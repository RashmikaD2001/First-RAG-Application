# ğŸ§  Retrieval-Augmented Generation (RAG) using LangChain + Google Gemini

This project demonstrates a simple **Retrieval-Augmented Generation (RAG)** pipeline built with **LangChain**, **Chroma**, and **Google Gemini**.

It loads Markdown (`.md`) documents, splits them into smaller chunks, stores them in a **Chroma vector database**, and uses **Gemini** to generate intelligent, context-based answers to user queries.

---

## ğŸš€ Features

- ğŸ—‚ï¸ Load and process Markdown documents from a local `data/` directory  
- ğŸ§© Split text into manageable overlapping chunks  
- ğŸ§  Generate embeddings using **Google Generative AI (Gemini Embeddings)**  
- ğŸ’¾ Store and retrieve document vectors using **Chroma**  
- ğŸ’¬ Generate answers with **Gemini 2.0 Flash** using retrieved context  
- âš™ï¸ Fully script-based workflow (no need for a web app or UI)

---

## ğŸ“ Project Structure
```
â”œâ”€â”€ data/ # Markdown files for embedding
â”œâ”€â”€ chroma/ # Persisted Chroma vector database
â”œâ”€â”€ .env # Contains GOOGLE_API_KEY
â”œâ”€â”€ main.py # Document loader, splitter, and DB builder
â””â”€â”€ query.py # Query interface using Gemini + Chroma
```

---

## ğŸ”‘ Requirements

- Python 3.10+
- [Google API key](https://aistudio.google.com/app/apikey)
